# 机器学习

1. 机器学习致力于研究如何通过计算的手段，利用经验来改善系统自身的性能。  
2. “模型”泛指从数据中学得的结果，在计算机上从数据中产生“模型”的算法为学习算法。  
3. 尽管训练集通常只是**样本空间**的一个很小的采样，我们仍希望它能很好地反映出样本空间的特性，否则很难期望在训练集上学得的模型能在整个样本空间上都工作得很好。  
4. 归纳是从特殊到一般的泛化过程，即从具体的事实归结出一般性规律。“从样例中学习”是一个归纳的过程，因此亦称“归纳学习”。  
5. 现实问题中我们常面临很大的假设空间，但学习的过程是基于有限样本训练集进行的，因此，可能有多个假设与训练集一致，即存在着一个与训练集一致的“假设集合”，我们称之为“版本空间”。
6. 机器学习算法在学习过程中对某种类型假设的偏好，称为“归纳偏好”。任何一个有效的机器学习算法必有其归纳偏好，归纳偏好可看作学习算法自身在一个可能很庞大的假设空间中对假设进行选择的启发式或“价值观”。
7. “奥卡姆剃刀”是一种常用的、自然科学研究中最基本的原则，即“若有多个假设与观察一致，则选最简单的那个”。
8. 大数据时代的三大关键技术：机器学习，云计算，众包。机器学习提供数据分析能力，云计算提供数据处理能力，众包提供数据标注能力。
9. 过拟合：把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质。欠拟合：对训练样本的一般性质尚未学好。过拟合是无法彻底避免的，我们所能做的只是“缓解”，或者说减小其风险。
10. 单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。
11. 为减小因样本划分不同而引入的差别，$k$折交叉验证通常要随机使用不同的划分重复$p$次，最终的评估结果是这$p$次$k$折交叉验证结果的均值。
12. 自助法在数据集较小、难以有效划分训练/测试集时很有用；此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。
13. 混淆矩阵，查准率，查全率，F1
14. P-R曲线，ROC曲线都是先把预测结果排序，然后依次截断；不同的是横纵坐标的度量有所差别。
15. 为权衡不同类型错误所造成的不同损失，可为错误赋予“非均等代价”。
16. 泛化误差可分解为偏差、方差与噪声之和。
    偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；
    方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；
    噪声则表达了在当前任务上任何学习算法所达到的期望泛化误差的下界，即刻画了学习问题本身的难度。
    偏差-方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。
17. 对数几率回归（logistic regression）：用线性回归模型的预测结果去逼近真实标记的对数几率。
17. LDA的思想非常朴素：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离。
18. 纠错输出码ECOC是将编码的思想引入类别拆分，并尽可能在解码过程中具有容错性。
19. 解决类别不平衡问题的三类做法：欠采样（EasyEnsemble），过采样，阈值移动。
20. 多标记学习(multi-label learning)：为一个样本同时预测出多个类别标记。
